{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgrad: 1 2 -2.0\n",
      "\tgrad: 2 4 -7.840000152587891\n",
      "\tgrad: 3 6 -16.228801727294922\n",
      "\tgrad: 4 8 -23.657981872558594\n",
      "\tgrad: 5 10 -25.136608123779297\n",
      "progress: 0 loss: 6.318490982055664 w: 1.748633861541748\n",
      "\tgrad: 1 2 -0.5027322769165039\n",
      "\tgrad: 2 4 -1.9707107543945312\n",
      "\tgrad: 3 6 -4.079372406005859\n",
      "\tgrad: 4 8 -5.946815490722656\n",
      "\tgrad: 5 10 -6.318492889404297\n",
      "progress: 1 loss: 0.39923352003097534 w: 1.9368151426315308\n",
      "\tgrad: 1 2 -0.12636971473693848\n",
      "\tgrad: 2 4 -0.49536895751953125\n",
      "\tgrad: 3 6 -1.0254135131835938\n",
      "\tgrad: 4 8 -1.4948234558105469\n",
      "\tgrad: 5 10 -1.5882492065429688\n",
      "progress: 2 loss: 0.025225356221199036 w: 1.9841175079345703\n",
      "\tgrad: 1 2 -0.031764984130859375\n",
      "\tgrad: 2 4 -0.12451839447021484\n",
      "\tgrad: 3 6 -0.2577552795410156\n",
      "\tgrad: 4 8 -0.3757476806640625\n",
      "\tgrad: 5 10 -0.3992271423339844\n",
      "progress: 3 loss: 0.001593823079019785 w: 1.9960076808929443\n",
      "\tgrad: 1 2 -0.007984638214111328\n",
      "\tgrad: 2 4 -0.031299591064453125\n",
      "\tgrad: 3 6 -0.06479072570800781\n",
      "\tgrad: 4 8 -0.09444808959960938\n",
      "\tgrad: 5 10 -0.10034561157226562\n",
      "progress: 4 loss: 0.00010069241398014128 w: 1.9989964962005615\n",
      "\tgrad: 1 2 -0.002007007598876953\n",
      "\tgrad: 2 4 -0.007867813110351562\n",
      "\tgrad: 3 6 -0.016284942626953125\n",
      "\tgrad: 4 8 -0.02374267578125\n",
      "\tgrad: 5 10 -0.025224685668945312\n",
      "progress: 5 loss: 6.36284767097095e-06 w: 1.9997477531433105\n",
      "\tgrad: 1 2 -0.0005044937133789062\n",
      "\tgrad: 2 4 -0.0019779205322265625\n",
      "\tgrad: 3 6 -0.004094123840332031\n",
      "\tgrad: 4 8 -0.005970001220703125\n",
      "\tgrad: 5 10 -0.0063419342041015625\n",
      "progress: 6 loss: 4.022012944915332e-07 w: 1.999936580657959\n",
      "\tgrad: 1 2 -0.00012683868408203125\n",
      "\tgrad: 2 4 -0.0004968643188476562\n",
      "\tgrad: 3 6 -0.0010271072387695312\n",
      "\tgrad: 4 8 -0.001499176025390625\n",
      "\tgrad: 5 10 -0.0015926361083984375\n",
      "progress: 7 loss: 2.5364897737745196e-08 w: 1.9999841451644897\n",
      "\tgrad: 1 2 -3.170967102050781e-05\n",
      "\tgrad: 2 4 -0.0001239776611328125\n",
      "\tgrad: 3 6 -0.0002574920654296875\n",
      "\tgrad: 4 8 -0.00037384033203125\n",
      "\tgrad: 5 10 -0.000400543212890625\n",
      "progress: 8 loss: 1.6043486539274454e-09 w: 1.9999960660934448\n",
      "\tgrad: 1 2 -7.867813110351562e-06\n",
      "\tgrad: 2 4 -3.0517578125e-05\n",
      "\tgrad: 3 6 -6.29425048828125e-05\n",
      "\tgrad: 4 8 -9.1552734375e-05\n",
      "\tgrad: 5 10 -9.5367431640625e-05\n",
      "progress: 9 loss: 9.094947017729282e-11 w: 1.9999990463256836\n",
      "\tgrad: 1 2 -1.9073486328125e-06\n",
      "\tgrad: 2 4 -7.62939453125e-06\n",
      "\tgrad: 3 6 -1.430511474609375e-05\n",
      "\tgrad: 4 8 -2.288818359375e-05\n",
      "\tgrad: 5 10 -1.9073486328125e-05\n",
      "progress: 10 loss: 3.637978807091713e-12 w: 1.999999761581421\n",
      "\tgrad: 1 2 -4.76837158203125e-07\n",
      "\tgrad: 2 4 -1.9073486328125e-06\n",
      "\tgrad: 3 6 -5.7220458984375e-06\n",
      "\tgrad: 4 8 -7.62939453125e-06\n",
      "\tgrad: 5 10 -9.5367431640625e-06\n",
      "progress: 11 loss: 9.094947017729282e-13 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 12 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 13 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 14 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 15 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 16 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 17 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 18 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 19 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 20 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 21 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 22 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 23 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 24 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 25 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 26 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 27 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 28 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 29 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 30 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 31 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 32 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 33 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 34 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 35 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 36 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 37 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 38 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 39 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 40 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 41 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 42 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 43 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 44 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 45 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 46 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 47 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 48 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 49 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 50 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 51 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 52 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 53 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 54 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 55 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 56 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 57 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 58 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 59 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 60 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 61 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 62 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 63 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 64 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 65 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 66 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 67 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 68 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 69 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 70 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 71 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 72 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 73 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 74 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 75 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 76 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 77 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 78 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 79 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 80 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 81 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 82 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 83 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 84 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 85 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 86 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 87 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 88 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 89 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 90 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 91 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 92 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 93 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 94 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 95 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 96 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 97 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 98 loss: 0.0 w: 2.0\n",
      "\tgrad: 1 2 0.0\n",
      "\tgrad: 2 4 0.0\n",
      "\tgrad: 3 6 0.0\n",
      "\tgrad: 4 8 0.0\n",
      "\tgrad: 5 10 0.0\n",
      "progress: 99 loss: 0.0 w: 2.0\n"
     ]
    }
   ],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "#仅仅使用pytorch的自动求导autograd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "w=Variable(torch.Tensor([1.0]), requires_grad=True)\n",
    "b=Variable(torch.Tensor([1.0]), requires_grad=True)\n",
    "\n",
    "#download training data\n",
    "x_data=[1,2,3,4,5]\n",
    "y_data=[2,4,6,8,10]\n",
    "\n",
    "#our model forward pass\n",
    "def forward(x):\n",
    "    #return x*w+b\n",
    "    return x*w\n",
    "\n",
    "#loss function\n",
    "def loss(x,y):\n",
    "    y_pred=forward(x)\n",
    "    return (y_pred-y)*(y_pred-y)\n",
    "\n",
    "epoch_list=[]\n",
    "w_list=[]\n",
    "l_list=[]\n",
    "#training loop\n",
    "for epoch in range(100):\n",
    "    for x_val,y_val in zip(x_data,y_data):\n",
    "        l=loss(x_val,y_val)\n",
    "        #对loss进行back propagation\n",
    "        l.backward()\n",
    "        print(\"\\tgrad:\",x_val,y_val,w.grad.data[0])\n",
    "        #更新参数\n",
    "        w.data=w.data-0.01*w.grad.data\n",
    "        #b.data=b.data-0.01*w.grad.data\n",
    "        \n",
    "        #manually zero the gradients after updating weights\n",
    "        w.grad.data.zero_()\n",
    "    epoch_list.append(epoch)\n",
    "    w_list.append(float(w.data[0]))\n",
    "    l_list.append(float(l.data[0]))\n",
    "    print(\"progress:\",epoch,\"loss:\",l.data[0],\"w:\",w.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEWCAYAAABG030jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1073f6588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epoch_list,l_list)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"loss\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch_list,w_list)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"weight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 122.08110046386719\n",
      "1 54.34931945800781\n",
      "2 24.19702911376953\n",
      "3 10.774045944213867\n",
      "4 4.798489570617676\n",
      "5 2.138309955596924\n",
      "6 0.9540427923202515\n",
      "7 0.42680901288986206\n",
      "8 0.19206970930099487\n",
      "9 0.08754134178161621\n",
      "10 0.04097852110862732\n",
      "11 0.020221540704369545\n",
      "12 0.01095248106867075\n",
      "13 0.006798129994422197\n",
      "14 0.0049211410805583\n",
      "15 0.004058282822370529\n",
      "16 0.003647265024483204\n",
      "17 0.003437880426645279\n",
      "18 0.003318646689876914\n",
      "19 0.003239793237298727\n",
      "20 0.003179381135851145\n",
      "21 0.0031275656074285507\n",
      "22 0.0030798616353422403\n",
      "23 0.0030343930702656507\n",
      "24 0.0029902211390435696\n",
      "25 0.0029470068402588367\n",
      "26 0.0029045504052191973\n",
      "27 0.0028627358842641115\n",
      "28 0.00282160472124815\n",
      "29 0.0027810228057205677\n",
      "30 0.002741053933277726\n",
      "31 0.002701665973290801\n",
      "32 0.0026628663763403893\n",
      "33 0.002624556655064225\n",
      "34 0.002586837625131011\n",
      "35 0.002549688331782818\n",
      "36 0.0025130058638751507\n",
      "37 0.002476939233019948\n",
      "38 0.002441331511363387\n",
      "39 0.0024062327574938536\n",
      "40 0.0023716455325484276\n",
      "41 0.0023375956807285547\n",
      "42 0.0023039760999381542\n",
      "43 0.0022708799224346876\n",
      "44 0.0022382212337106466\n",
      "45 0.0022060503251850605\n",
      "46 0.0021743669640272856\n",
      "47 0.0021431175991892815\n",
      "48 0.0021123054903000593\n",
      "49 0.002081965794786811\n",
      "50 0.002052019350230694\n",
      "51 0.00202252808958292\n",
      "52 0.0019934966694563627\n",
      "53 0.001964827300980687\n",
      "54 0.001936598215252161\n",
      "55 0.0019087489927187562\n",
      "56 0.0018813054775819182\n",
      "57 0.0018542661564424634\n",
      "58 0.0018276412738487124\n",
      "59 0.0018013371154665947\n",
      "60 0.0017754711443558335\n",
      "61 0.0017499455716460943\n",
      "62 0.0017248291987925768\n",
      "63 0.0017000287771224976\n",
      "64 0.0016755935503169894\n",
      "65 0.0016514887101948261\n",
      "66 0.0016277885297313333\n",
      "67 0.0016043877694755793\n",
      "68 0.0015813459176570177\n",
      "69 0.0015586039517074823\n",
      "70 0.001536197611130774\n",
      "71 0.0015141208423301578\n",
      "72 0.0014923365088179708\n",
      "73 0.0014709262177348137\n",
      "74 0.0014497725060209632\n",
      "75 0.0014289346290752292\n",
      "76 0.0014084179420024157\n",
      "77 0.001388155622407794\n",
      "78 0.0013682207791134715\n",
      "79 0.0013485539238899946\n",
      "80 0.001329154591076076\n",
      "81 0.0013100578216835856\n",
      "82 0.0012912412639707327\n",
      "83 0.0012726625427603722\n",
      "84 0.0012543975608423352\n",
      "85 0.0012363443383947015\n",
      "86 0.0012185985688120127\n",
      "87 0.0012010798091068864\n",
      "88 0.0011838419595733285\n",
      "89 0.0011668180814012885\n",
      "90 0.0011500456603243947\n",
      "91 0.0011335277231410146\n",
      "92 0.0011172167723998427\n",
      "93 0.0011011590249836445\n",
      "94 0.0010853421408683062\n",
      "95 0.0010697499383240938\n",
      "96 0.0010543743846938014\n",
      "97 0.001039218856021762\n",
      "98 0.00102427345700562\n",
      "99 0.0010095564648509026\n",
      "100 0.0009950450621545315\n",
      "101 0.0009807496098801494\n",
      "102 0.0009666571277193725\n",
      "103 0.000952758127823472\n",
      "104 0.0009390578488819301\n",
      "105 0.0009255805052816868\n",
      "106 0.0009122744086198509\n",
      "107 0.0008991605136543512\n",
      "108 0.0008862455142661929\n",
      "109 0.0008735079900361598\n",
      "110 0.0008609599899500608\n",
      "111 0.0008485841099172831\n",
      "112 0.0008363784290850163\n",
      "113 0.0008243563934229314\n",
      "114 0.0008124959422275424\n",
      "115 0.0008008218137547374\n",
      "116 0.0007893204456195235\n",
      "117 0.0007779962616041303\n",
      "118 0.0007668150938116014\n",
      "119 0.0007557756034657359\n",
      "120 0.000744906603358686\n",
      "121 0.0007342030294239521\n",
      "122 0.0007236659876070917\n",
      "123 0.0007132687605917454\n",
      "124 0.0007030116394162178\n",
      "125 0.000692900619469583\n",
      "126 0.0006829509511590004\n",
      "127 0.0006731291068717837\n",
      "128 0.000663445913232863\n",
      "129 0.0006539322785101831\n",
      "130 0.000644524407107383\n",
      "131 0.0006352681666612625\n",
      "132 0.0006261342205107212\n",
      "133 0.0006171255954541266\n",
      "134 0.0006082740146666765\n",
      "135 0.0005995163228362799\n",
      "136 0.0005909125320613384\n",
      "137 0.0005824158433824778\n",
      "138 0.0005740586784668267\n",
      "139 0.0005657923757098615\n",
      "140 0.0005576642579399049\n",
      "141 0.0005496508674696088\n",
      "142 0.0005417523207142949\n",
      "143 0.0005339552299119532\n",
      "144 0.0005262920167297125\n",
      "145 0.0005187332280911505\n",
      "146 0.000511279096826911\n",
      "147 0.0005039193201810122\n",
      "148 0.0004966867854818702\n",
      "149 0.0004895554156973958\n",
      "150 0.0004825151991099119\n",
      "151 0.0004755855770781636\n",
      "152 0.0004687287728302181\n",
      "153 0.00046200916403904557\n",
      "154 0.00045537285041064024\n",
      "155 0.0004488264094106853\n",
      "156 0.0004423580248840153\n",
      "157 0.00043600081698969007\n",
      "158 0.0004297463165130466\n",
      "159 0.00042357173515483737\n",
      "160 0.00041749284719116986\n",
      "161 0.00041148747550323606\n",
      "162 0.00040557238389737904\n",
      "163 0.00039974431274458766\n",
      "164 0.00039400113746523857\n",
      "165 0.00038833500002510846\n",
      "166 0.0003827486070804298\n",
      "167 0.0003772490017581731\n",
      "168 0.0003718194493558258\n",
      "169 0.0003664748801384121\n",
      "170 0.00036121319863013923\n",
      "171 0.0003560202894732356\n",
      "172 0.0003509161761030555\n",
      "173 0.0003458716382738203\n",
      "174 0.00034089366090483963\n",
      "175 0.0003360018599778414\n",
      "176 0.0003311634063720703\n",
      "177 0.00032640312565490603\n",
      "178 0.00032171900966204703\n",
      "179 0.0003170977288391441\n",
      "180 0.0003125345683656633\n",
      "181 0.00030804937705397606\n",
      "182 0.00030361785320565104\n",
      "183 0.00029925163835287094\n",
      "184 0.0002949520421680063\n",
      "185 0.0002907132438849658\n",
      "186 0.00028653565095737576\n",
      "187 0.0002824176335707307\n",
      "188 0.000278356543276459\n",
      "189 0.0002743576478678733\n",
      "190 0.0002704134676605463\n",
      "191 0.0002665329957380891\n",
      "192 0.0002627003414090723\n",
      "193 0.0002589272626210004\n",
      "194 0.0002552065416239202\n",
      "195 0.00025153858587145805\n",
      "196 0.00024791547912172973\n",
      "197 0.00024436411331407726\n",
      "198 0.00024084918550215662\n",
      "199 0.00023739195603411645\n",
      "200 0.00023398197663482279\n",
      "201 0.00023061168030835688\n",
      "202 0.00022729826741851866\n",
      "203 0.00022403491311706603\n",
      "204 0.00022080815688241273\n",
      "205 0.00021764423581771553\n",
      "206 0.00021451845532283187\n",
      "207 0.0002114381641149521\n",
      "208 0.00020839186618104577\n",
      "209 0.00020540434343274683\n",
      "210 0.0002024499699473381\n",
      "211 0.00019953672017436475\n",
      "212 0.0001966633426491171\n",
      "213 0.00019384396728128195\n",
      "214 0.00019105382671114057\n",
      "215 0.00018830224871635437\n",
      "216 0.00018560307216830552\n",
      "217 0.00018293678294867277\n",
      "218 0.00018031086074188352\n",
      "219 0.0001777225115802139\n",
      "220 0.00017516137450002134\n",
      "221 0.00017264936468563974\n",
      "222 0.0001701661094557494\n",
      "223 0.00016771441732998937\n",
      "224 0.00016530216089449823\n",
      "225 0.0001629378239158541\n",
      "226 0.00016058390610851347\n",
      "227 0.00015828365576453507\n",
      "228 0.00015601236373186111\n",
      "229 0.00015376758528873324\n",
      "230 0.00015155624714680016\n",
      "231 0.00014937302330508828\n",
      "232 0.00014722959895152599\n",
      "233 0.00014510657638311386\n",
      "234 0.00014302469207905233\n",
      "235 0.00014097581151872873\n",
      "236 0.00013894993753638119\n",
      "237 0.0001369434903608635\n",
      "238 0.00013498742191586643\n",
      "239 0.00013303739251568913\n",
      "240 0.00013113018940202892\n",
      "241 0.00012924597831442952\n",
      "242 0.00012739119119942188\n",
      "243 0.0001255569513887167\n",
      "244 0.00012375283404253423\n",
      "245 0.00012197718024253845\n",
      "246 0.00012022138253087178\n",
      "247 0.0001184972861665301\n",
      "248 0.00011679633462335914\n",
      "249 0.00011511641059769318\n",
      "250 0.00011345919483574107\n",
      "251 0.00011183058813912794\n",
      "252 0.00011021620593965054\n",
      "253 0.00010863778879866004\n",
      "254 0.00010707676847232506\n",
      "255 0.00010553184256423265\n",
      "256 0.0001040194692905061\n",
      "257 0.00010252273932565004\n",
      "258 0.00010104852844960988\n",
      "259 9.959892486222088e-05\n",
      "260 9.816959936870262e-05\n",
      "261 9.67546075116843e-05\n",
      "262 9.536123252473772e-05\n",
      "263 9.400050475960597e-05\n",
      "264 9.264118853025138e-05\n",
      "265 9.131174738286063e-05\n",
      "266 9.000350837595761e-05\n",
      "267 8.870635065250099e-05\n",
      "268 8.743708895053715e-05\n",
      "269 8.617914863862097e-05\n",
      "270 8.494102803524584e-05\n",
      "271 8.371931471629068e-05\n",
      "272 8.25170282041654e-05\n",
      "273 8.132394577842206e-05\n",
      "274 8.015672938199714e-05\n",
      "275 7.901036588009447e-05\n",
      "276 7.787071081111208e-05\n",
      "277 7.675461529288441e-05\n",
      "278 7.564708357676864e-05\n",
      "279 7.456717867171392e-05\n",
      "280 7.349404040724039e-05\n",
      "281 7.2432158049196e-05\n",
      "282 7.139764056773856e-05\n",
      "283 7.036910392343998e-05\n",
      "284 6.93596521159634e-05\n",
      "285 6.83608595863916e-05\n",
      "286 6.737265357514843e-05\n",
      "287 6.641342770308256e-05\n",
      "288 6.545685027958825e-05\n",
      "289 6.451515946537256e-05\n",
      "290 6.35853866697289e-05\n",
      "291 6.267571006901562e-05\n",
      "292 6.177030445542186e-05\n",
      "293 6.088917507440783e-05\n",
      "294 6.0010323068127036e-05\n",
      "295 5.914547000429593e-05\n",
      "296 5.8295765484217554e-05\n",
      "297 5.7459699746686965e-05\n",
      "298 5.66358030482661e-05\n",
      "299 5.58222018298693e-05\n",
      "300 5.5020529543980956e-05\n",
      "301 5.4227210057433695e-05\n",
      "302 5.3448584367288277e-05\n",
      "303 5.2678544307127595e-05\n",
      "304 5.192498065298423e-05\n",
      "305 5.1176022680010647e-05\n",
      "306 5.0441994972061366e-05\n",
      "307 4.9719012167770416e-05\n",
      "308 4.900532076135278e-05\n",
      "309 4.829961835639551e-05\n",
      "310 4.760344381793402e-05\n",
      "311 4.692267975769937e-05\n",
      "312 4.624445136869326e-05\n",
      "313 4.558175351121463e-05\n",
      "314 4.4923835957888514e-05\n",
      "315 4.4283471652306616e-05\n",
      "316 4.364309279480949e-05\n",
      "317 4.3019965232815593e-05\n",
      "318 4.2401316022733226e-05\n",
      "319 4.179240931989625e-05\n",
      "320 4.119052391615696e-05\n",
      "321 4.0595594327896833e-05\n",
      "322 4.001602064818144e-05\n",
      "323 3.944061973015778e-05\n",
      "324 3.88722910429351e-05\n",
      "325 3.831707363133319e-05\n",
      "326 3.7767644244013354e-05\n",
      "327 3.7224664993118495e-05\n",
      "328 3.6686677049146965e-05\n",
      "329 3.6162036849418655e-05\n",
      "330 3.5638058761833236e-05\n",
      "331 3.512444891384803e-05\n",
      "332 3.462379754637368e-05\n",
      "333 3.41236918757204e-05\n",
      "334 3.363362338859588e-05\n",
      "335 3.3154134143842384e-05\n",
      "336 3.2675099646439776e-05\n",
      "337 3.220581857021898e-05\n",
      "338 3.174123776261695e-05\n",
      "339 3.1289790058508515e-05\n",
      "340 3.0839641112834215e-05\n",
      "341 3.0395000067073852e-05\n",
      "342 2.9953589546494186e-05\n",
      "343 2.9523929697461426e-05\n",
      "344 2.9098317099851556e-05\n",
      "345 2.868513183784671e-05\n",
      "346 2.8272126655792817e-05\n",
      "347 2.7863035938935354e-05\n",
      "348 2.7465150196803734e-05\n",
      "349 2.7070123906014487e-05\n",
      "350 2.6678266294766217e-05\n",
      "351 2.629612026794348e-05\n",
      "352 2.591880547697656e-05\n",
      "353 2.5548026314936578e-05\n",
      "354 2.5181099772453308e-05\n",
      "355 2.4815955839585513e-05\n",
      "356 2.4462100554956123e-05\n",
      "357 2.410878732916899e-05\n",
      "358 2.3760319891152903e-05\n",
      "359 2.3418884666170925e-05\n",
      "360 2.3085503926267847e-05\n",
      "361 2.2750078642275184e-05\n",
      "362 2.2421791072702035e-05\n",
      "363 2.2102172806626186e-05\n",
      "364 2.1784035197924823e-05\n",
      "365 2.1473593733389862e-05\n",
      "366 2.1160301912459545e-05\n",
      "367 2.085966116283089e-05\n",
      "368 2.055959339486435e-05\n",
      "369 2.0262745238142088e-05\n",
      "370 1.9974031602032483e-05\n",
      "371 1.9685845472849905e-05\n",
      "372 1.940384754561819e-05\n",
      "373 1.9124900063616224e-05\n",
      "374 1.884722223621793e-05\n",
      "375 1.857582901720889e-05\n",
      "376 1.8311129679204896e-05\n",
      "377 1.8047838238999248e-05\n",
      "378 1.7787191609386355e-05\n",
      "379 1.7532573110656813e-05\n",
      "380 1.728293500491418e-05\n",
      "381 1.7032933101290837e-05\n",
      "382 1.6786420019343495e-05\n",
      "383 1.6544770915061235e-05\n",
      "384 1.6309335478581488e-05\n",
      "385 1.6074423911049962e-05\n",
      "386 1.5841913409531116e-05\n",
      "387 1.5614310541423038e-05\n",
      "388 1.539040749776177e-05\n",
      "389 1.5168350728345104e-05\n",
      "390 1.495240030635614e-05\n",
      "391 1.4737998753844295e-05\n",
      "392 1.4526696759276092e-05\n",
      "393 1.4316920896817464e-05\n",
      "394 1.411238463333575e-05\n",
      "395 1.3907801985624246e-05\n",
      "396 1.370772770314943e-05\n",
      "397 1.3509958080248907e-05\n",
      "398 1.3316168406163342e-05\n",
      "399 1.3126095836923923e-05\n",
      "400 1.2936766324855853e-05\n",
      "401 1.275150862056762e-05\n",
      "402 1.25677970572724e-05\n",
      "403 1.2386642083583865e-05\n",
      "404 1.2208224688947666e-05\n",
      "405 1.203110150527209e-05\n",
      "406 1.185987548524281e-05\n",
      "407 1.1690080100379419e-05\n",
      "408 1.1522689419507515e-05\n",
      "409 1.1356704817444552e-05\n",
      "410 1.1193088539585005e-05\n",
      "411 1.1033944247174077e-05\n",
      "412 1.087421605916461e-05\n",
      "413 1.0718696103140246e-05\n",
      "414 1.0564295735093765e-05\n",
      "415 1.0412329174869228e-05\n",
      "416 1.0261464012728538e-05\n",
      "417 1.0114289580087643e-05\n",
      "418 9.969275197363459e-06\n",
      "419 9.824765584198758e-06\n",
      "420 9.686194061941933e-06\n",
      "421 9.546273759042379e-06\n",
      "422 9.40737209020881e-06\n",
      "423 9.273026080336422e-06\n",
      "424 9.140174370259047e-06\n",
      "425 9.0089788500336e-06\n",
      "426 8.879252163751516e-06\n",
      "427 8.750125743972603e-06\n",
      "428 8.627403985883575e-06\n",
      "429 8.500129297317471e-06\n",
      "430 8.380357940040994e-06\n",
      "431 8.258765774371568e-06\n",
      "432 8.140713362081442e-06\n",
      "433 8.023184818739537e-06\n",
      "434 7.90797912486596e-06\n",
      "435 7.79588026489364e-06\n",
      "436 7.682005161768757e-06\n",
      "437 7.571525202365592e-06\n",
      "438 7.463435395038687e-06\n",
      "439 7.355648449447472e-06\n",
      "440 7.250211638165638e-06\n",
      "441 7.146158168325201e-06\n",
      "442 7.043319783406332e-06\n",
      "443 6.941227184142917e-06\n",
      "444 6.842918082838878e-06\n",
      "445 6.743350695614936e-06\n",
      "446 6.6455622800276615e-06\n",
      "447 6.5504200392751954e-06\n",
      "448 6.457441941165598e-06\n",
      "449 6.364251021295786e-06\n",
      "450 6.2736280597164296e-06\n",
      "451 6.182356628414709e-06\n",
      "452 6.094905984355137e-06\n",
      "453 6.0053762354073115e-06\n",
      "454 5.919335308135487e-06\n",
      "455 5.836161108163651e-06\n",
      "456 5.751346179749817e-06\n",
      "457 5.66950529901078e-06\n",
      "458 5.586876795860007e-06\n",
      "459 5.506219622475328e-06\n",
      "460 5.426690222520847e-06\n",
      "461 5.350293577066623e-06\n",
      "462 5.2719024097314104e-06\n",
      "463 5.196208803681657e-06\n",
      "464 5.122378752275836e-06\n",
      "465 5.049992523709079e-06\n",
      "466 4.976046511728782e-06\n",
      "467 4.904705292574363e-06\n",
      "468 4.8347737902076915e-06\n",
      "469 4.7642042773077264e-06\n",
      "470 4.695285497291479e-06\n",
      "471 4.627744601748418e-06\n",
      "472 4.561562491289806e-06\n",
      "473 4.496964720601682e-06\n",
      "474 4.432585228641983e-06\n",
      "475 4.368669578980189e-06\n",
      "476 4.3043733057857025e-06\n",
      "477 4.2441452023922466e-06\n",
      "478 4.182797965768259e-06\n",
      "479 4.122603058931418e-06\n",
      "480 4.064486347488128e-06\n",
      "481 4.004920810984913e-06\n",
      "482 3.94695143768331e-06\n",
      "483 3.890434527420439e-06\n",
      "484 3.83478072762955e-06\n",
      "485 3.7806589716637973e-06\n",
      "486 3.7251274989102967e-06\n",
      "487 3.671787908388069e-06\n",
      "488 3.6192748211760772e-06\n",
      "489 3.5660461890074657e-06\n",
      "490 3.516038987072534e-06\n",
      "491 3.4649813187570544e-06\n",
      "492 3.4150493775086943e-06\n",
      "493 3.3651590456429403e-06\n",
      "494 3.3183882806042675e-06\n",
      "495 3.270263960075681e-06\n",
      "496 3.22249115924933e-06\n",
      "497 3.176726295350818e-06\n",
      "498 3.130364348180592e-06\n",
      "499 3.0866910947224824e-06\n",
      "predict (after training) 4 11.997981071472168\n"
     ]
    }
   ],
   "source": [
    "#使用pytorch编程风格实现linear regression\n",
    "'''\n",
    "1 design your model using class with Variable\n",
    "2 construct loss and optimizer(select from PyTorch API)\n",
    "3 training cycl(forward,backward,update)\n",
    "'''\n",
    "#data definition (3 data *1 value)\n",
    "x_data=Variable(torch.Tensor([[1.0],[2.0],[3.0]]))\n",
    "y_data=Variable(torch.Tensor([[3.0],[6.0],[9.0]]))\n",
    "\n",
    "#model class in pytorch way\n",
    "class Module(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In this constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Module,self).__init__()\n",
    "        self.linear=torch.nn.Linear(1,1) #one in one out\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data.We can use modules defined in the constructor as \n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred=self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "#our model\n",
    "model=Module()\n",
    "\n",
    "#Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "#in the SGD constructor will contain the learnable parameters of the two\n",
    "#nn.Linear modules which are members of the model.\n",
    "\n",
    "criterion=torch.nn.MSELoss(size_average=False)\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "\n",
    "#training : forward loss backward step\n",
    "for epoch in range(500):\n",
    "    #forward pass: compute predicted y by passing x to the model\n",
    "    y_pred= model (x_data)\n",
    "    \n",
    "    #compute and print loss\n",
    "    loss=criterion(y_pred,y_data)\n",
    "    print(epoch,loss.data[0])\n",
    "    \n",
    "    #Zero gradients, perform a backward pass , and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #update the paremeters\n",
    "    optimizer.step()\n",
    "\n",
    "#after training\n",
    "hour_var=Variable(torch.Tensor([[4.0]]))\n",
    "print(\"predict (after training)\",4,model.forward(hour_var).data[0][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image classifier example\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#the output of torchvision datasets are PILImage images of range [0,1]\n",
    "#We transform them to Tensor of normalized range[-1,1]\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  cat truck horse   cat\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#funcitons to show an image\n",
    "def imshow(img):\n",
    "    img=img/2+0.5 #unnormalize\n",
    "    npimg=img.numpy()\n",
    "    plt.imshow(np.transpose(npimg,(1,2,0)))\n",
    "    \n",
    "#get some random training images\n",
    "dataiter=iter(trainloader)\n",
    "images,labels=dataiter.next()\n",
    "\n",
    "#show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "#print labels\n",
    "print(' '.join('%5s' %classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1=nn.Conv2d(3,6,5)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        self.fc1=nn.Linear(16 * 5 * 5,120)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.fc3=nn.Linear(84,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=x.view(-1,16 * 5 * 5)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net=Net()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.SGD(net.parameters(),lr=0.01,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.088\n",
      "[1,  4000] loss: 1.940\n",
      "[1,  6000] loss: 1.939\n",
      "[1,  8000] loss: 1.933\n",
      "[1, 10000] loss: 1.945\n",
      "[1, 12000] loss: 1.962\n",
      "[2,  2000] loss: 1.933\n",
      "[2,  4000] loss: 1.919\n",
      "[2,  6000] loss: 1.972\n",
      "[2,  8000] loss: 1.969\n",
      "[2, 10000] loss: 1.981\n",
      "[2, 12000] loss: 1.968\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:    cat  ship  ship plane\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(Variable(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:    cat  ship  ship  ship\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 26 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane :  9 %\n",
      "Accuracy of   car : 13 %\n",
      "Accuracy of  bird :  4 %\n",
      "Accuracy of   cat : 17 %\n",
      "Accuracy of  deer : 12 %\n",
      "Accuracy of   dog : 17 %\n",
      "Accuracy of  frog : 39 %\n",
      "Accuracy of horse : 43 %\n",
      "Accuracy of  ship : 56 %\n",
      "Accuracy of truck : 48 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7b748db2b046>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.5.3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device_id)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mcopied\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \"\"\"\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;31m# Variables stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mcopied\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \"\"\"\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.3/lib/python3.5/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.3/lib/python3.5/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;31m# We need this method only for lazy init, so we can remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_CudaBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.3/lib/python3.5/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m         raise RuntimeError(\n\u001b[1;32m     83\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_sparse_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.3/lib/python3.5/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "net.cuda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
